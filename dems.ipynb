{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Democratic Candidates\n",
    "This notebook analyzes the sentiment of the tweets posted for democratic candidates. There are a few ideas that i need to consider. For instance,\n",
    "\n",
    "1. The number of positive and negative tweets posted for each candidate\n",
    "2. The proportion of positivity and negativity\n",
    "3. How these change over time and possibly after each debate or major event\n",
    "4. Is there any relationship between the tweets sentiments and the pols?\n",
    "5. The location of the sentiments, broken down to the states, possibly focusing on the swing states.\n",
    "6. We can expand the analysis beyond the tweets and to the *users/voters*.\n",
    "7. Make a word cloud for the tweets about each candidate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweeter Data\n",
    "I start with pulling some data from the major candidates from twitter, Elizabeth Warren, Bernine Sanders, and Joe Biden. For this, I used *tweepy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch api keys from api_keys.py\n",
    "\n",
    "from api_keys import *\n",
    "\n",
    "query = {'@BernieSanders': 'Bernie Sanders',     # Bernie\n",
    "         '@ewarren':       'Elizabeth Warren',   # Elizabeth\n",
    "         '@KamalaHarris':  'Kamala Harris',      # Kamala\n",
    "         '@PeteButtigieg': 'Pete Buttigieg',     # Pete\n",
    "         '@JoeBiden':      'Joe Biden',          # Joe Biden\n",
    "         }\n",
    "\n",
    "query = dict((k.lower(), v.lower()) for k,v in query.items())\n",
    "\n",
    "# make everything lowercase for consitency\n",
    "mentions = list(query.keys())\n",
    "names = list(query.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Things that are need to be taken care of in data cleaning.\n",
    "\n",
    "* Remove links (https, etc.).\n",
    "* Remove pnctuations except dots and commas. So later we break them down accordingly. Or maybe *tokenize* does that automatically? Need to check!\n",
    "* Remove special characters\n",
    "* Remove numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the tweet\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as pp\n",
    "#nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions\n",
    "\n",
    "# only remove URL, reserved words, emojies, and smilies. Preserve hashtags and mentions\n",
    "pp.set_options(pp.OPT.URL, pp.OPT.RESERVED, pp.OPT.EMOJI, pp.OPT.SMILEY)\n",
    "# save stop words to be removed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# nltk tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# For the contractions\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "\n",
    "# remove mentions if not a candidate\n",
    "def remove_mentions(tweet, mentions):\n",
    "    words = tweet.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.startswith('@'):\n",
    "            candidate_mentioned = False\n",
    "            for m in mentions:\n",
    "                if m in w:\n",
    "                    candidate_mentioned = True\n",
    "                    clean_words.append(w.replace(m, query[m].lower()))\n",
    "                    break\n",
    "            if not candidate_mentioned:\n",
    "                clean_words.append(w[1:]) # remove @\n",
    "        else:\n",
    "            clean_words.append(w)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "# expand hashtags\n",
    "def fix_hashtags(tweet):\n",
    "    # first replace underscore with space\n",
    "    tweet = tweet.replace('_', ' ')\n",
    "    words = tweet.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.startswith('#'):\n",
    "            w = ' '.join([a for a in re.split('([A-Z][a-z]+)', w[1:]) if a])\n",
    "        clean_words.append(w)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "def expand_contractions(tweet):\n",
    "    def replace(match):\n",
    "        # expand the contraction with the most possible alternative : [0]\n",
    "        return contractions[match.group(0)][0]\n",
    "    return contractions_re.sub(replace, tweet)\n",
    "    \n",
    "def clean_tweet(tweet):\n",
    "    # remove URL, Reserved words (RT, FAV, etc.), Emojies, Smilies, and Numbers.\n",
    "    # preserve mentions and hastags for now\n",
    "    tweet = pp.clean(tweet)\n",
    "    # fix hashtags\n",
    "    tweet = fix_hashtags(tweet)\n",
    "    # make the tweet lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # now remove mentions that are not the candidates\n",
    "    tweet = remove_mentions(tweet, mentions)\n",
    "    # conver U+2019 to U+0027 (apostrophe)\n",
    "    tweet = tweet.replace(u\"\\u2019\", u\"\\u0027\")\n",
    "    # expand the contractions\n",
    "    tweet = expand_contractions(tweet)\n",
    "    # remove 's\n",
    "    tweet = tweet.replace(\"'s\",'')\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    # break into sentences\n",
    "    # tb = TextBlob(tweet)\n",
    "    sentences = []\n",
    "    for sent in tokenizer.tokenize(tweet):\n",
    "#    for sent in tb.sentences: # for this punkt package of nltk has to be downloaded once\n",
    "#                              # with the following code:\n",
    "#                              # import nltk\n",
    "#                              # nltk.download('punkt')\n",
    "        sent = str(sent)\n",
    "        # remove ponctuations\n",
    "        sent = sent.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        # consolidate white spaces\n",
    "        sent = ' '.join(sent.split())\n",
    "        if len(sent) > 4: # if the sentence is larger than 4 chars\n",
    "            sentences.append(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Each tweet might comprise multiple sentences. Therefore each tweet must be broken down to different sentences with *tokenize* functionality of *TextBlob*. The final sentiment can be a function of the sentiment of different sentences, perhaps the average (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# This needs more work to be more accurate. Some ideas:\n",
    "#    1. Don't remove emoticons and use vader\n",
    "#    2. clean stopwrds and everything else, and use outofthebox texblob\n",
    "#    3. Read papers on political sentiment analysis with twitter\n",
    "def get_sentiment(text, mode = 'textblob'):\n",
    "    if mode == 'textblob':\n",
    "        testimonial = TextBlob(text)\n",
    "        return {'pol': testimonial.sentiment.polarity,\n",
    "                'subj': testimonial.sentiment.subjectivity}\n",
    "    elif mode == 'nltk':\n",
    "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        return sid.polarity_scores(text)\n",
    "    elif mode == 'api':    \n",
    "        import requests   \n",
    "        # api-endpoint \n",
    "        URL = \"http://text-processing.com/api/sentiment/\"\n",
    "        params = {'text':text}\n",
    "        r = requests.post(url = URL, data = params)\n",
    "        data = r.json()\n",
    "        return(data['probability'])\n",
    "    elif mode == 'vader':\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        return analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet and Sentence Objects\n",
    "Let's makes some classes and methods for tweets and sentences that cleans and gets the sentiment of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence object\n",
    "import numpy as np\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentiment = []\n",
    "        self.sentimentize()\n",
    "    \n",
    "    # calculate sentiment for the sentence\n",
    "    def sentimentize(self, compare = True):\n",
    "        if compare:\n",
    "            self.sentiment = dict()\n",
    "            for mode in ['nltk', 'vader', 'textblob', 'api']:\n",
    "                self.sentiment[mode] = get_sentiment(self.text, mode)\n",
    "        else:\n",
    "            self.sentiment = {'textblob': get_sentiment(self.text, mode = 'textblob')}\n",
    "            \n",
    "    def __str__(self):\n",
    "        import json\n",
    "        sentiment_str = ''\n",
    "        for s in self.sentiment:\n",
    "            sentiment_str += s + ': ' + json.dumps(self.sentiment[s]) + '\\n'\n",
    "        return '%s >>>>> \\n%s' % (self.text, sentiment_str)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s >>>>> Pol: %.1f (Sub: %.1f)' % (self.text, self.polarity, self.subjectivity)\n",
    "        \n",
    "# tweet object\n",
    "class Tweet:\n",
    "    def __init__(self, text, time):\n",
    "        self.time = time\n",
    "        self.text = text\n",
    "        self.sentiment = None\n",
    "        self.sentencize()\n",
    "        \n",
    "    # clean tweet and break down sentences\n",
    "    def sentencize(self):\n",
    "        self.sentences = [Sentence(t) for t in clean_tweet(self.text)]\n",
    "        # tweet snetiment is the avergae sentiment of all sentences. TODO: Might not be correct!\n",
    "        self.sentiment = (np.mean([s.sentiment['textblob']['pol'] for s in self.sentences]),\n",
    "                                  [s.sentiment['textblob']['pol'] for s in self.sentences])\n",
    "    \n",
    "    def disp(self):\n",
    "        print('********************')\n",
    "        print(self.text)\n",
    "        print('====================')\n",
    "        for sentence in self.sentences:\n",
    "            print(sentence)\n",
    "            print('--------------------')\n",
    "        print('Compount Sentiment:', self.sentiment)\n",
    "        print('********************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet downloader\n",
    "This downloads tweets to test clearning and sentiment analyis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST API\n",
    "import tweepy\n",
    "import sys\n",
    "\n",
    "def get_sample_tweets(query_phrase, tweet_count):\n",
    "    # authorization\n",
    "    auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True,           # wait until the limit is replenished\n",
    "                           wait_on_rate_limit_notify=True)    # reply with a message if the limit is reached\n",
    "\n",
    "    # check if not authorized\n",
    "    if (not api):\n",
    "        print (\"Can't Authenticate\")\n",
    "        return\n",
    "\n",
    "    tweets = []\n",
    "    for status in tweepy.Cursor(api.search, q = query_phrase,\n",
    "                                        tweet_mode = 'extended',\n",
    "                                        lang = 'en').items(tweet_count):\n",
    "        try:\n",
    "            full_text = status._json['retweeted_status']['full_text']\n",
    "        except:\n",
    "            full_text = status._json['full_text']\n",
    "            \n",
    "        ts = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(status._json['created_at'],'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "        tweets.append(Tweet(full_text, ts))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Now let's download some tweets and test the *cleaning* and *sentiment analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      ".@BernieSanders held a town hall earlier to discuss his plan to eliminate medical debt. One woman, who struggles to pay medical bills bc her son has Type 1 Diabetes and  her daughter has Rheumatoid Arthritis told Sanders, “there's no way out for us. I mean other than you Bernie.”\n",
      "====================\n",
      "berniesanders held a town hall earlier to discuss his plan to eliminate medical debt >>>>> \n",
      "nltk: {\"neg\": 0.172, \"neu\": 0.828, \"pos\": 0.0, \"compound\": -0.3612}\n",
      "vader: {\"neg\": 0.172, \"neu\": 0.828, \"pos\": 0.0, \"compound\": -0.3612}\n",
      "textblob: {\"pol\": 0.0, \"subj\": 0.25}\n",
      "api: {\"neg\": 0.3370813282836256, \"neutral\": 0.9075629305170034, \"pos\": 0.6629186717163744}\n",
      "\n",
      "--------------------\n",
      "one woman who struggles to pay medical bills bc her son has type 1 diabetes and her daughter has rheumatoid arthritis told sanders there is no way out for us >>>>> \n",
      "nltk: {\"neg\": 0.19, \"neu\": 0.81, \"pos\": 0.0, \"compound\": -0.6249}\n",
      "vader: {\"neg\": 0.19, \"neu\": 0.81, \"pos\": 0.0, \"compound\": -0.6249}\n",
      "textblob: {\"pol\": 0.0, \"subj\": 0.0}\n",
      "api: {\"neg\": 0.3675577675576016, \"neutral\": 0.9976035904012036, \"pos\": 0.6324422324423984}\n",
      "\n",
      "--------------------\n",
      "i mean other than you bernie >>>>> \n",
      "nltk: {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}\n",
      "vader: {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}\n",
      "textblob: {\"pol\": -0.21875, \"subj\": 0.53125}\n",
      "api: {\"neg\": 0.6572065965670553, \"neutral\": 0.1631383663109808, \"pos\": 0.3427934034329447}\n",
      "\n",
      "--------------------\n",
      "Compount Sentiment: (-0.07291666666666667, [0.0, 0.0, -0.21875])\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "tweets = get_sample_tweets(query_phrase = 'bernie sanders', tweet_count = 1)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet.disp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "Now let's stream the tweets in real time and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STREAMING\n",
    "\n",
    "import tweepy\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "auth = OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACS_TOKEN, ACS_SECRET)\n",
    "api = API(auth, wait_on_rate_limit=True,\n",
    "                wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "# Continue with rest of code\n",
    "\n",
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, max_count = 5, verbose = False):\n",
    "        from datetime import datetime\n",
    "        self.count = 0\n",
    "        self.verbose = verbose\n",
    "        self.max_count = max_count\n",
    "        self.last_window = datetime.utcnow()\n",
    "        self.window_count = 0\n",
    "        self.window_sentiment = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_text(tweet):\n",
    "        try:\n",
    "            if 'retweeted_status' in tweet:\n",
    "                try:\n",
    "                    text = tweet['retweeted_status']['extended_tweet']['full_text']\n",
    "                except:\n",
    "                    text = tweet['retweeted_status']['text']\n",
    "            else:\n",
    "                try:\n",
    "                    text = tweet['extended_tweet']['full_text']\n",
    "                except:\n",
    "                    text = tweet['text']\n",
    "            return text\n",
    "        except:\n",
    "            return\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_time(tweet):\n",
    "        from datetime import datetime\n",
    "        #strftime('%Y-%m-%d %H:%M:%S',\n",
    "        return datetime.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "    def update_agg_sents(self, tweet):\n",
    "        from datetime import datetime\n",
    "        import numpy as np\n",
    "        diff = (tweet.time - self.last_window).total_seconds()\n",
    "        if diff < 30:\n",
    "            self.window_count += 1\n",
    "            self.window_sentiment.append(tweet.sentiment[0])\n",
    "        else:\n",
    "            print('Start Time:', self.last_window)\n",
    "            print('Count:', self.window_count)\n",
    "            print('Avg Sentiment:', np.mean(self.window_sentiment))\n",
    "            self.last_window = datetime.utcnow()\n",
    "            self.window_count = 0\n",
    "            self.window_sentiment = []\n",
    "        \n",
    "    def disp(self, data, tweet):\n",
    "        if self.verbose:\n",
    "            print('*************')\n",
    "            print(self.count)\n",
    "            print('*************')\n",
    "            print(data['text'])\n",
    "            print('~~~~~~~~~~~~~')\n",
    "            tweet.disp()\n",
    "\n",
    "        \n",
    "    def on_data(self, data):\n",
    "        data = json.loads(data)\n",
    "        text = self.get_text(data)\n",
    "        timestamp = self.get_time(data)\n",
    "        tweet = Tweet(text, timestamp)\n",
    "        # if verbose, print the breakdown\n",
    "        print((tweet.time - self.last_window).total_seconds(), tweet.sentiment)\n",
    "        self.disp(data, tweet)\n",
    "        self.update_agg_sents(tweet)\n",
    "        \n",
    "        self.count += 1\n",
    "        # finish if the greated than count threshold\n",
    "        if self.count > self.max_count:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print('ERR!!')\n",
    "        print(status)\n",
    "        if status == 420:\n",
    "            return False\n",
    "        \n",
    "    def on_status(self, status):\n",
    "        print(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-25 00:24:27 (0.0, [0.0])\n",
      "*************\n",
      "0\n",
      "*************\n",
      "In related news, Tulsi Gabbard will be spending campaign funds to acquire a seat on the #45 bus to Glendale.\n",
      "~~~~~~~~~~~~~\n",
      "********************\n",
      "In related news, Tulsi Gabbard will be spending campaign funds to acquire a seat on the #45 bus to Glendale.\n",
      "====================\n",
      "in related news tulsi gabbard will be spending campaign funds to acquire a seat on the 45 bus to glendale >>>>> \n",
      "nltk: {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}\n",
      "vader: {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}\n",
      "textblob: {\"pol\": 0.0, \"subj\": 0.4}\n",
      "api: {\"neg\": 0.47058098744145793, \"neutral\": 0.7906073241042647, \"pos\": 0.5294190125585421}\n",
      "\n",
      "--------------------\n",
      "Compount Sentiment: (0.0, [0.0])\n",
      "********************\n",
      "6:59:54.809765\n",
      "2019-09-25 00:24:27 (0.025, [0.0, 0.0, 0.0, 0.1])\n",
      "*************\n",
      "1\n",
      "*************\n",
      "RT @SteveGuest: Embarrassed for Chuck Todd.\n",
      "\n",
      "On Joe Biden's relationship w/ his son: Hunter \"has been weaponized politically. I challenge a…\n",
      "~~~~~~~~~~~~~\n",
      "********************\n",
      "Embarrassed for Chuck Todd.\n",
      "\n",
      "On Joe Biden's relationship w/ his son: Hunter \"has been weaponized politically. I challenge any parent to deal with that.\"\n",
      "\n",
      "Hunter's a grown man who had no experience working with Ukraine or with natural gas but was paid $50k a month by Burisma. https://t.co/o5S0kVSJyI\n",
      "====================\n",
      "embarrassed for chuck todd >>>>> \n",
      "nltk: {\"neg\": 0.455, \"neu\": 0.545, \"pos\": 0.0, \"compound\": -0.3612}\n",
      "vader: {\"neg\": 0.455, \"neu\": 0.545, \"pos\": 0.0, \"compound\": -0.3612}\n",
      "textblob: {\"pol\": 0.0, \"subj\": 0.0}\n",
      "api: {\"neg\": 0.5117853684931202, \"neutral\": 0.5936334389821091, \"pos\": 0.4882146315068798}\n",
      "\n",
      "--------------------\n",
      "on joe biden relationship w his son hunter has been weaponized politically >>>>> \n",
      "nltk: {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}\n",
      "vader: {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}\n",
      "textblob: {\"pol\": 0.0, \"subj\": 0.1}\n",
      "api: {\"neg\": 0.36103853970737954, \"neutral\": 0.996300568673055, \"pos\": 0.6389614602926205}\n",
      "\n",
      "--------------------\n",
      "i challenge any parent to deal with that >>>>> \n",
      "nltk: {\"neg\": 0.0, \"neu\": 0.822, \"pos\": 0.178, \"compound\": 0.0772}\n",
      "vader: {\"neg\": 0.0, \"neu\": 0.822, \"pos\": 0.178, \"compound\": 0.0772}\n",
      "textblob: {\"pol\": 0.0, \"subj\": 0.0}\n",
      "api: {\"neg\": 0.3425619958074255, \"neutral\": 0.6561422949928194, \"pos\": 0.6574380041925745}\n",
      "\n",
      "--------------------\n",
      "hunter a grown man who had no eerience working with ukraine or with natural gas but was paid 50k a month by burisma >>>>> \n",
      "nltk: {\"neg\": 0.072, \"neu\": 0.85, \"pos\": 0.078, \"compound\": 0.0387}\n",
      "vader: {\"neg\": 0.072, \"neu\": 0.85, \"pos\": 0.078, \"compound\": 0.0387}\n",
      "textblob: {\"pol\": 0.1, \"subj\": 0.4}\n",
      "api: {\"neg\": 0.4808003301856565, \"neutral\": 0.7127079770116961, \"pos\": 0.5191996698143435}\n",
      "\n",
      "--------------------\n",
      "Compount Sentiment: (0.025, [0.0, 0.0, 0.0, 0.1])\n",
      "********************\n",
      "6:59:54.809765\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "max_count = 1      \n",
    "myStreamListener = MyStreamListener(max_count = max_count, verbose = True)\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n",
    "\n",
    "try:\n",
    "    myStream.filter(track = mentions + names, languages=['en'])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped.\")\n",
    "finally:\n",
    "    print('Done.')\n",
    "    myStream.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam and plot\n",
    "Try streaming and plotting the results on the go for **Bernie Sanders** only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.972578 (0.0, [0.0, 0.0])\n",
      "-4.972578 (0.0, [0.0, 0.0, 0.0, 0.0])\n",
      "-4.972578 (0.0, [0.0])\n",
      "-3.972578 (-0.1875, [-0.1875])\n",
      "-3.972578 (0.0, [0.0, 0.0, 0.0])\n",
      "-2.972578 (-0.6, [-0.6])\n",
      "-1.972578 (-0.07500000000000001, [0.0, -0.30000000000000004, 0.0, 0.0])\n",
      "-1.972578 (0.33116883116883117, [0.33116883116883117])\n",
      "-1.972578 (1.0, [1.0])\n",
      "-1.972578 (0.23333333333333336, [0.0, 0.3, 0.4000000000000001])\n",
      "-0.972578 (0.0, [0.0])\n",
      "-0.972578 (0.0, [0.0, 0.0, 0.0, 0.0])\n",
      "-0.972578 (0.0, [0.0, 0.0, 0.0, 0.0])\n",
      "-0.972578 (-0.16666666666666669, [-0.5, 0.16666666666666666])\n",
      "0.027422 (0.08333333333333334, [0.08333333333333334])\n",
      "0.027422 (0.016666666666666666, [0.03333333333333333, 0.0])\n",
      "0.027422 (0.0875, [0.35, 0.0, 0.0, 0.0])\n",
      "3.027422 (0.0625, [0.0, 0.125])\n",
      "3.027422 (0.25555555555555554, [0.25555555555555554])\n",
      "3.027422 (0.25, [0.0, 0.5])\n",
      "3.027422 (-0.5249999999999999, [-0.3499999999999999, -0.6999999999999998])\n",
      "4.027422 (0.35, [0.0, 0.7])\n",
      "4.027422 (0.0, [0.0])\n",
      "5.027422 (0.0, [0.0])\n",
      "5.027422 (-0.16666666666666666, [-0.16666666666666666])\n",
      "5.027422 (0.5, [0.0, 1.0])\n",
      "6.027422 (-0.07777777777777779, [-0.15555555555555559, 0.0])\n",
      "6.027422 (0.0, [0.5, 0.0, 0.0, -0.5])\n",
      "6.027422 (0.0625, [0.25, 0.0, 0.0, 0.0])\n",
      "6.027422 (0.35, [0.35])\n",
      "7.027422 (0.0875, [0.175, 0.0])\n",
      "7.027422 (-0.16666666666666666, [-0.16666666666666666])\n",
      "7.027422 (0.08, [0.0, 0.0, 0.0, 0.0, 0.4])\n",
      "3.027422 (0.0, [0.0])\n",
      "8.027422 (0.10476190476190476, [0.10476190476190476])\n",
      "9.027422 (0.0, [0.0])\n",
      "10.027422 (0.0, [0.0, 0.0])\n",
      "10.027422 (0.0, [0.0, 0.0, 0.0])\n",
      "10.027422 (0.0, [0.0])\n",
      "10.027422 (0.2333333333333333, [0.7, 0.0, 0.0])\n",
      "10.027422 (-0.09722222222222221, [-0.09722222222222221])\n",
      "11.027422 (-0.1875, [-0.1875])\n",
      "12.027422 (0.0, [0.0])\n",
      "12.027422 (0.13061224489795917, [0.13061224489795917])\n",
      "10.027422 (0.2841346153846154, [0.2, 0.3682692307692308])\n",
      "13.027422 (0.0, [0.0])\n",
      "13.027422 (0.0, [0.0])\n",
      "14.027422 (0.15, [0.15])\n",
      "14.027422 (0.0, [0.0])\n",
      "14.027422 (0.0, [0.0])\n",
      "14.027422 (-0.2916666666666667, [-0.2916666666666667])\n",
      "14.027422 (0.0, [0.0])\n",
      "14.027422 (0.0, [0.0])\n",
      "14.027422 (0.2, [0.2])\n",
      "14.027422 (0.06666666666666667, [0.2, 0.0, 0.0])\n",
      "15.027422 (0.0, [0.0])\n",
      "15.027422 (-0.16666666666666669, [-0.5, 0.16666666666666666])\n",
      "15.027422 (0.15833333333333333, [0.475, 0.0, 0.0])\n",
      "16.027422 (0.16, [0.0, 0.0, 0.0, 0.8, 0.0])\n",
      "16.027422 (-0.29750000000000004, [-0.29750000000000004])\n",
      "17.027422 (0.14545454545454545, [0.14545454545454545])\n",
      "17.027422 (-0.07500000000000001, [0.0, -0.30000000000000004, 0.0, 0.0])\n",
      "18.027422 (0.05555555555555555, [0.16666666666666666, 0.0, 0.0])\n",
      "18.027422 (0.6, [0.6])\n",
      "18.027422 (0.0, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "19.027422 (0.0, [0.0])\n",
      "19.027422 (0.0, [0.0])\n",
      "19.027422 (0.0, [0.0, 0.0, 0.0])\n",
      "20.027422 (0.10606060606060606, [0.11818181818181818, 0.0, 0.2])\n",
      "20.027422 (0.0, [0.0])\n",
      "20.027422 (0.525, [0.75, 0.3])\n",
      "20.027422 (0.25, [0.25])\n",
      "21.027422 (0.35, [0.7, 0.0])\n",
      "22.027422 (0.06796875, [0.0, 0.271875, 0.0, 0.0])\n",
      "22.027422 (-0.05, [0.0, -0.1])\n",
      "22.027422 (0.15000000000000002, [0.30000000000000004, 0.0])\n",
      "22.027422 (0.0, [0.0])\n",
      "22.027422 (0.0, [0.0])\n",
      "22.027422 (0.10606060606060606, [0.11818181818181818, 0.0, 0.2])\n",
      "22.027422 (0.109375, [0.0, 0.0, 0.0, 0.4375])\n",
      "22.027422 (0.0, [0.0])\n",
      "23.027422 (0.0, [0.0])\n",
      "24.027422 (0.35833333333333334, [0.5375, 0.5375, 0.0])\n",
      "24.027422 (-0.0638888888888889, [0.1, -0.2916666666666667, 0.0])\n",
      "25.027422 (0.0, [0.0, 0.0])\n",
      "25.027422 (0.5, [0.5])\n",
      "25.027422 (-0.2777777777777778, [0.0, -0.8, -0.033333333333333326])\n",
      "26.027422 (-0.08124999999999999, [0.0, -0.16249999999999998])\n",
      "26.027422 (0.0, [0.0, 0.0])\n",
      "26.027422 (0.041666666666666664, [0.25, 0.0, -0.125])\n",
      "26.027422 (0.0, [0.0])\n",
      "26.027422 (0.10000000000000002, [0.10000000000000002])\n",
      "27.027422 (0.5, [0.5])\n",
      "27.027422 (0.0, [0.0])\n",
      "27.027422 (0.0, [0.0])\n",
      "28.027422 (0.13333333333333333, [0.26666666666666666, 0.0])\n",
      "29.027422 (0.0, [0.0, 0.0])\n",
      "29.027422 (-0.1875, [-0.1875])\n",
      "30.027422 (0.0, [0.0, 0.0, 0.0])\n",
      "Start Time: 2019-09-25 01:00:53.972578\n",
      "Count: 98\n",
      "Avg Sentiment: 0.05709651239694061\n",
      "-7.213592 (0.0, [0.0])\n",
      "-7.213592 (0.75, [0.8, 0.7])\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "max_count = 100\n",
    "myStreamListener = MyStreamListener(max_count = max_count)\n",
    "myStream = tweepy.Stream(auth = api.auth, listener = myStreamListener)\n",
    "\n",
    "try:\n",
    "    myStream.filter(track = ['Bernie Sanders', '@berniesanders'], languages=['en'])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped.\")\n",
    "finally:\n",
    "    print('Done.')\n",
    "    myStream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
