{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Democratic Candidates\n",
    "This notebook analyzes the sentiment of the tweets posted for democratic candidates. There are a few ideas that i need to consider. For instance,\n",
    "\n",
    "1. The number of positive and negative tweets posted for each candidate\n",
    "2. The proportion of positivity and negativity\n",
    "3. How these change over time and possibly after each debate or major event\n",
    "4. Is there any relationship between the tweets sentiments and the pols?\n",
    "5. The location of the sentiments, broken down to the states, possibly focusing on the swing states.\n",
    "6. We can expand the analysis beyond the tweets and to the *users/voters*.\n",
    "7. Make a word cloud for the tweets about each candidate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweeter Data\n",
    "I start with pulling some data from the major candidates from twitter, Elizabeth Warren, Bernine Sanders, and Joe Biden. For this, I used *tweepy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch api keys from api_keys.py\n",
    "\n",
    "from api_keys import *\n",
    "\n",
    "query = {'@BernieSanders': 'Bernie Sanders',     # Bernie\n",
    "         '@ewarren':       'Elizabeth Warren',   # Elizabeth\n",
    "         '@KamalaHarris':  'Kamala Harris',      # Kamala\n",
    "         '@PeteButtigieg': 'Pete Buttigieg',     # Pete\n",
    "         '@JoeBiden':      'Joe Biden',          # Joe Biden\n",
    "         }\n",
    "\n",
    "query = dict((k.lower(), v.lower()) for k,v in query.items())\n",
    "\n",
    "# make everything lowercase for consitency\n",
    "mentions = query.keys()\n",
    "names = query.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Things that are need to be taken care of in data cleaning.\n",
    "\n",
    "* Remove links (https, etc.).\n",
    "* Remove pnctuations except dots and commas. So later we break them down accordingly. Or maybe *tokenize* does that automatically? Need to check!\n",
    "* Remove special characters\n",
    "* Remove numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the tweet\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as pp\n",
    "#nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions\n",
    "\n",
    "# only remove URL, reserved words, emojies, and smilies. Preserve hashtags and mentions\n",
    "pp.set_options(pp.OPT.URL, pp.OPT.RESERVED, pp.OPT.EMOJI, pp.OPT.SMILEY)\n",
    "# save stop words to be removed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# nltk tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# For the contractions\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "\n",
    "# remove mentions if not a candidate\n",
    "def remove_mentions(tweet, mentions):\n",
    "    words = tweet.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.startswith('@'):\n",
    "            candidate_mentioned = False\n",
    "            for m in mentions:\n",
    "                if m in w:\n",
    "                    candidate_mentioned = True\n",
    "                    clean_words.append(w.replace(m, query[m].lower()))\n",
    "                    break\n",
    "            if not candidate_mentioned:\n",
    "                clean_words.append(w[1:]) # remove @\n",
    "        else:\n",
    "            clean_words.append(w)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "# expand hashtags\n",
    "def fix_hashtags(tweet):\n",
    "    # first replace underscore with space\n",
    "    tweet = tweet.replace('_', ' ')\n",
    "    words = tweet.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.startswith('#'):\n",
    "            w = ' '.join([a for a in re.split('([A-Z][a-z]+)', w[1:]) if a])\n",
    "        clean_words.append(w)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "def expand_contractions(tweet):\n",
    "    def replace(match):\n",
    "        # expand the contraction with the most possible alternative : [0]\n",
    "        return contractions[match.group(0)][0]\n",
    "    return contractions_re.sub(replace, tweet)\n",
    "    \n",
    "def clean_tweet(tweet):\n",
    "    # remove URL, Reserved words (RT, FAV, etc.), Emojies, Smilies, and Numbers.\n",
    "    # preserve mentions and hastags for now\n",
    "    tweet = pp.clean(tweet)\n",
    "    # fix hashtags\n",
    "    tweet = fix_hashtags(tweet)\n",
    "    # make the tweet lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # now remove mentions that are not the candidates\n",
    "    tweet = remove_mentions(tweet, mentions)\n",
    "    # conver U+2019 to U+0027 (apostrophe)\n",
    "    tweet = tweet.replace(u\"\\u2019\", u\"\\u0027\")\n",
    "    # expand the contractions\n",
    "    tweet = expand_contractions(tweet)\n",
    "    # remove 's\n",
    "    tweet = tweet.replace(\"'s\",'')\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    # break into sentences\n",
    "    # tb = TextBlob(tweet)\n",
    "    sentences = []\n",
    "    for sent in tokenizer.tokenize(tweet):\n",
    "#    for sent in tb.sentences: # for this punkt package of nltk has to be downloaded once\n",
    "#                              # with the following code:\n",
    "#                              # import nltk\n",
    "#                              # nltk.download('punkt')\n",
    "        sent = str(sent)\n",
    "        # remove ponctuations\n",
    "        sent = sent.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        # consolidate white spaces\n",
    "        sent = ' '.join(sent.split())\n",
    "        if len(sent) > 4: # if the sentence is larger than 4 chars\n",
    "            sentences.append(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Each tweet might comprise multiple sentences. Therefore each tweet must be broken down to different sentences with *tokenize* functionality of *TextBlob*. The final sentiment can be a function of the sentiment of different sentences, perhaps the average (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# This needs more work to be more accurate. Some ideas:\n",
    "#    1. Don't remove emoticons and use vader\n",
    "#    2. clean stopwrds and everything else, and use outofthebox texblob\n",
    "#    3. Read papers on political sentiment analysis with twitter\n",
    "def get_sentiment(text, mode = 'textblob'):\n",
    "    if mode == 'textblob':\n",
    "        testimonial = TextBlob(text)\n",
    "        return {'Pol': testimonial.sentiment.polarity,\n",
    "                'Subj': testimonial.sentiment.subjectivity}\n",
    "    elif mode == 'nltk':\n",
    "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        return sid.polarity_scores(text)\n",
    "    elif mode == 'api':    \n",
    "        import requests   \n",
    "        # api-endpoint \n",
    "        URL = \"http://text-processing.com/api/sentiment/\"\n",
    "        params = {'text':text}\n",
    "        r = requests.post(url = URL, data = params)\n",
    "        data = r.json()\n",
    "        return(data['probability'])\n",
    "    elif mode == 'vader':\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        return analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet and Sentence Objects\n",
    "Let's makes some classes and methods for tweets and sentences that cleans and gets the sentiment of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence object\n",
    "class Sentence:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentiment = []\n",
    "        self.sentimentize()\n",
    "    \n",
    "    # calculate sentiment for the sentence\n",
    "    def sentimentize(self):\n",
    "        for mode in ['nltk', 'vader', 'textblob', 'api']:\n",
    "            self.sentiment.append((mode, get_sentiment(self.text, mode)))\n",
    "            \n",
    "    def __str__(self):\n",
    "        import json\n",
    "        sentiment_str = ''\n",
    "        for s in self.sentiment:\n",
    "            sentiment_str += json.dumps(s) + '\\n'\n",
    "        return '%s >>>>> \\n%s' % (self.text, sentiment_str)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s >>>>> Pol: %.1f (Sub: %.1f)' % (self.text, self.polarity, self.subjectivity)\n",
    "        \n",
    "# tweet object\n",
    "class Tweet:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentencize()\n",
    "        \n",
    "    # clean tweet and break down sentences\n",
    "    def sentencize(self):\n",
    "        self.sentences = [Sentence(t) for t in clean_tweet(self.text)]\n",
    "    \n",
    "    def disp(self):\n",
    "        print('********************')\n",
    "        print(self.text)\n",
    "        print('====================')\n",
    "        for sentence in self.sentences:\n",
    "            print(sentence)\n",
    "            print('--------------------')\n",
    "        print('********************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet downloader\n",
    "This downloads tweets to test clearning and sentiment analyis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST API\n",
    "import tweepy\n",
    "import sys\n",
    "\n",
    "def get_sample_tweets(query_phrase, tweet_count):\n",
    "    # authorization\n",
    "    auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True,           # wait until the limit is replenished\n",
    "                           wait_on_rate_limit_notify=True)    # reply with a message if the limit is reached\n",
    "\n",
    "    # check if not authorized\n",
    "    if (not api):\n",
    "        print (\"Can't Authenticate\")\n",
    "        return\n",
    "\n",
    "    tweets = []\n",
    "    for status in tweepy.Cursor(api.search, q = query_phrase,\n",
    "                                        tweet_mode = 'extended',\n",
    "                                        lang = 'en').items(tweet_count):\n",
    "        try:\n",
    "            full_text = status._json['retweeted_status']['full_text']\n",
    "        except:\n",
    "            full_text = status._json['full_text']\n",
    "        \n",
    "        tweets.append(Tweet(full_text))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Now let's download some tweets and test the *cleaning* and *sentiment analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "There are a number of similarities between Bernie Sanders and Hulk Hogan. I wonâ€™t elaborate on this for some time...\n",
      "====================\n",
      "there are a number of similarities between bernie sanders and hulk hogan >>>>> \n",
      "[\"nltk\", {\"neg\": 0.0, \"neu\": 0.885, \"pos\": 0.115, \"compound\": 0.0772}]\n",
      "[\"vader\", {\"neg\": 0.0, \"neu\": 0.885, \"pos\": 0.115, \"compound\": 0.0772}]\n",
      "[\"textblob\", {\"Pol\": 0.0, \"Subj\": 0.0}]\n",
      "[\"api\", {\"neg\": 0.37125424236150817, \"neutral\": 0.8253129065161494, \"pos\": 0.6287457576384918}]\n",
      "\n",
      "--------------------\n",
      "i will not elaborate on this for some time >>>>> \n",
      "[\"nltk\", {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}]\n",
      "[\"vader\", {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}]\n",
      "[\"textblob\", {\"Pol\": -0.25, \"Subj\": 1.0}]\n",
      "[\"api\", {\"neg\": 0.5175747047924295, \"neutral\": 0.04378182270699219, \"pos\": 0.48242529520757044}]\n",
      "\n",
      "--------------------\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "tweets = get_sample_tweets(query_phrase = 'bernie sanders', tweet_count = 1)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet.disp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "Now let's stream the tweets in real time and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STREAMING\n",
    "\n",
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "auth = OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACS_TOKEN, ACS_SECRET)\n",
    "api = API(auth, wait_on_rate_limit=True,\n",
    "                wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "# Continue with rest of code\n",
    "\n",
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, max_count = 5):\n",
    "        self.count = 0\n",
    "        self.max_count = max_count\n",
    "\n",
    "    def on_data(self, data):\n",
    "        all_data = json.loads(data)\n",
    "        print('*************')\n",
    "        print(self.count)\n",
    "        print('*************')\n",
    "        #print('-----------')\n",
    "        print(all_data['text'])\n",
    "        print('||||||||')\n",
    "        try:\n",
    "            if 'retweeted_status' in all_data:\n",
    "                try:\n",
    "                    text = all_data['retweeted_status']['extended_tweet']['full_text']\n",
    "                except:\n",
    "                    text = all_data['retweeted_status']['text']\n",
    "            else:\n",
    "                try:\n",
    "                    text = all_data['extended_tweet']['full_text']\n",
    "                except:\n",
    "                    text = all_data['text']\n",
    "            tweet = Tweet(text)\n",
    "            tweet.disp()\n",
    "        except:\n",
    "            print('New structure!')\n",
    "        \n",
    "        self.count += 1\n",
    "        if self.count > self.max_count:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print('ERR!!')\n",
    "        print(status)\n",
    "        if status == 420:\n",
    "            return False\n",
    "        \n",
    "    def on_status(self, status):\n",
    "        print(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************\n",
      "0\n",
      "*************\n",
      "RT @NancyJKoch: Ilhan Omar ended Joe Bidenâ€™s Presidential campaign with one confession - Well of course!! At this point in time â¦@Ilhanâ© noâ€¦\n",
      "||||||||\n",
      "********************\n",
      "Ilhan Omar ended Joe Bidenâ€™s Presidential campaign with one confession - Well of course!! At this point in time â¦@Ilhanâ© not sure if this helps or hurts â¦â¦@JoeBidenâ© run. Pretty much another nail in the coffinðŸ’¥ https://t.co/uMjtEx2yP4\n",
      "====================\n",
      "ilhan omar ended joe biden presidential campaign with one confession well of course >>>>> \n",
      "[\"nltk\", {\"neg\": 0.0, \"neu\": 0.851, \"pos\": 0.149, \"compound\": 0.2732}]\n",
      "[\"vader\", {\"neg\": 0.0, \"neu\": 0.851, \"pos\": 0.149, \"compound\": 0.2732}]\n",
      "[\"textblob\", {\"Pol\": 0.0, \"Subj\": 0.0}]\n",
      "[\"api\", {\"neg\": 0.3191407847009151, \"neutral\": 0.7887384300136802, \"pos\": 0.6808592152990849}]\n",
      "\n",
      "--------------------\n",
      "at this point in time ilhan not sure if this helps or hurts joebiden run >>>>> \n",
      "[\"nltk\", {\"neg\": 0.257, \"neu\": 0.61, \"pos\": 0.132, \"compound\": -0.3532}]\n",
      "[\"vader\", {\"neg\": 0.257, \"neu\": 0.61, \"pos\": 0.132, \"compound\": -0.3532}]\n",
      "[\"textblob\", {\"Pol\": -0.25, \"Subj\": 0.8888888888888888}]\n",
      "[\"api\", {\"neg\": 0.6501175212703764, \"neutral\": 0.09387911424825716, \"pos\": 0.34988247872962364}]\n",
      "\n",
      "--------------------\n",
      "pretty much another nail in the coffin >>>>> \n",
      "[\"nltk\", {\"neg\": 0.0, \"neu\": 0.652, \"pos\": 0.348, \"compound\": 0.4939}]\n",
      "[\"vader\", {\"neg\": 0.0, \"neu\": 0.652, \"pos\": 0.348, \"compound\": 0.4939}]\n",
      "[\"textblob\", {\"Pol\": 0.225, \"Subj\": 0.6}]\n",
      "[\"api\", {\"neg\": 0.5996754403412933, \"neutral\": 0.20061083022335838, \"pos\": 0.4003245596587067}]\n",
      "\n",
      "--------------------\n",
      "********************\n",
      "*************\n",
      "1\n",
      "*************\n",
      "RT @RWPUSA: Finally, a presidential candidate, â¦@ewarrenâ©, with the courage to tell â¦@SpeakerPelosiâ© that sheâ€™s being a darned fool about iâ€¦\n",
      "||||||||\n",
      "********************\n",
      "Finally, a presidential candidate, â¦@ewarrenâ©, with the courage to tell â¦@SpeakerPelosiâ© that sheâ€™s being a darned fool about impeachment.\n",
      "The time is now!\n",
      "\n",
      "Warren repeats call for impeachment, accuses Congress of complicit. https://t.co/47KNpQ9KvD\n",
      "====================\n",
      "finally a presidential candidate ewarren with the courage to tell speakerpelosi that she is being a darned fool about impeachment >>>>> \n",
      "[\"nltk\", {\"neg\": 0.131, \"neu\": 0.724, \"pos\": 0.145, \"compound\": 0.0772}]\n",
      "[\"vader\", {\"neg\": 0.131, \"neu\": 0.724, \"pos\": 0.145, \"compound\": 0.0772}]\n",
      "[\"textblob\", {\"Pol\": 0.0, \"Subj\": 1.0}]\n",
      "[\"api\", {\"neg\": 0.29116841040380315, \"neutral\": 0.9253511304273573, \"pos\": 0.7088315895961969}]\n",
      "\n",
      "--------------------\n",
      "the time is now >>>>> \n",
      "[\"nltk\", {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}]\n",
      "[\"vader\", {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}]\n",
      "[\"textblob\", {\"Pol\": 0.0, \"Subj\": 0.0}]\n",
      "[\"api\", {\"neg\": 0.4164762610145688, \"neutral\": 0.8665898190631062, \"pos\": 0.5835237389854312}]\n",
      "\n",
      "--------------------\n",
      "warren repeats call for impeachment accuses congress of complicit >>>>> \n",
      "[\"nltk\", {\"neg\": 0.231, \"neu\": 0.769, \"pos\": 0.0, \"compound\": -0.34}]\n",
      "[\"vader\", {\"neg\": 0.231, \"neu\": 0.769, \"pos\": 0.0, \"compound\": -0.34}]\n",
      "[\"textblob\", {\"Pol\": 0.0, \"Subj\": 0.0}]\n",
      "[\"api\", {\"neg\": 0.5245220641655142, \"neutral\": 0.5581654340110623, \"pos\": 0.47547793583448583}]\n",
      "\n",
      "--------------------\n",
      "********************\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "max_count = 1      \n",
    "myStreamListener = MyStreamListener(max_count = max_count)\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n",
    "\n",
    "try:\n",
    "    myStream.filter(track = mentions, languages=['en'])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped.\")\n",
    "finally:\n",
    "    print('Done.')\n",
    "    myStream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
