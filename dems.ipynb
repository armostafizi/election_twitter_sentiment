{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Democratic Candidates\n",
    "This notebook analyzes the sentiment of the tweets posted for democratic candidates. There are a few ideas that i need to consider. For instance,\n",
    "\n",
    "1. The number of positive and negative tweets posted for each candidate\n",
    "2. The proportion of positivity and negativity\n",
    "3. How these change over time and possibly after each debate or major event\n",
    "4. Is there any relationship between the tweets sentiments and the pols?\n",
    "5. The location of the sentiments, broken down to the states, possibly focusing on the swing states.\n",
    "6. We can expand the analysis beyond the tweets and to the *users/voters*.\n",
    "7. Make a word cloud for the tweets about each candidate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweeter Data\n",
    "I start with pulling some data from the major candidates from twitter, Elizabeth Warren, Bernine Sanders, and Joe Biden. For this, I used *tweepy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch api keys from api_keys.py\n",
    "\n",
    "from api_keys import *\n",
    "\n",
    "query = {'@BernieSanders': 'Bernie Sanders',     # Bernie\n",
    "         '@ewarren':       'Elizabeth Warren',   # Elizabeth\n",
    "         '@KamalaHarris':  'Kamala Harris',      # Kamala\n",
    "         '@PeteButtigieg': 'Pete Buttigieg',     # Pete\n",
    "         '@JoeBiden':      'Joe Biden',          # Joe Biden\n",
    "         }\n",
    "\n",
    "query = dict((k.lower(), v.lower()) for k,v in query.items())\n",
    "\n",
    "# make everything lowercase for consitency\n",
    "mentions = query.keys()\n",
    "names = query.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Things that are need to be taken care of in data cleaning.\n",
    "\n",
    "* Remove links (https, etc.).\n",
    "* Remove pnctuations except dots and commas. So later we break them down accordingly. Or maybe *tokenize* does that automatically? Need to check!\n",
    "* Remove special characters\n",
    "* Remove numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the tweet\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as pp\n",
    "#nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions\n",
    "\n",
    "# only remove URL, reserved words, emojies, and smilies. Preserve hashtags and mentions\n",
    "pp.set_options(pp.OPT.URL, pp.OPT.RESERVED, pp.OPT.EMOJI, pp.OPT.SMILEY)\n",
    "# save stop words to be removed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# nltk tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# For the contractions\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "\n",
    "# remove mentions if not a candidate\n",
    "def remove_mentions(tweet, mentions):\n",
    "    words = tweet.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.startswith('@'):\n",
    "            candidate_mentioned = False\n",
    "            for m in mentions:\n",
    "                if m in w:\n",
    "                    candidate_mentioned = True\n",
    "                    clean_words.append(w.replace(m, query[m].lower()))\n",
    "                    break\n",
    "            if not candidate_mentioned:\n",
    "                clean_words.append(w[1:]) # remove @\n",
    "        else:\n",
    "            clean_words.append(w)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "# expand hashtags\n",
    "def fix_hashtags(tweet):\n",
    "    # first replace underscore with space\n",
    "    tweet = tweet.replace('_', ' ')\n",
    "    words = tweet.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.startswith('#'):\n",
    "            w = ' '.join([a for a in re.split('([A-Z][a-z]+)', w[1:]) if a])\n",
    "        clean_words.append(w)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "def expand_contractions(tweet):\n",
    "    def replace(match):\n",
    "        # expand the contraction with the most possible alternative : [0]\n",
    "        return contractions[match.group(0)][0]\n",
    "    return contractions_re.sub(replace, tweet)\n",
    "    \n",
    "def clean_tweet(tweet):\n",
    "    # remove URL, Reserved words (RT, FAV, etc.), Emojies, Smilies, and Numbers.\n",
    "    # preserve mentions and hastags for now\n",
    "    tweet = pp.clean(tweet)\n",
    "    # fix hashtags\n",
    "    tweet = fix_hashtags(tweet)\n",
    "    # make the tweet lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # now remove mentions that are not the candidates\n",
    "    tweet = remove_mentions(tweet, mentions)\n",
    "    # conver U+2019 to U+0027 (apostrophe)\n",
    "    tweet = tweet.replace(u\"\\u2019\", u\"\\u0027\")\n",
    "    # expand the contractions\n",
    "    tweet = expand_contractions(tweet)\n",
    "    # remove 's\n",
    "    tweet = tweet.replace(\"'s\",'')\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    # break into sentences\n",
    "    # tb = TextBlob(tweet)\n",
    "    sentences = []\n",
    "    for sent in tokenizer.tokenize(tweet):\n",
    "#    for sent in tb.sentences: # for this punkt package of nltk has to be downloaded once\n",
    "#                              # with the following code:\n",
    "#                              # import nltk\n",
    "#                              # nltk.download('punkt')\n",
    "        sent = str(sent)\n",
    "        # remove ponctuations\n",
    "        sent = sent.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        # consolidate white spaces\n",
    "        sent = ' '.join(sent.split())\n",
    "        if len(sent) > 4: # if the sentence is larger than 4 chars\n",
    "            sentences.append(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Each tweet might comprise multiple sentences. Therefore each tweet must be broken down to different sentences with *tokenize* functionality of *TextBlob*. The final sentiment can be a function of the sentiment of different sentences, perhaps the average (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk\n",
      "{'neg': 0.0, 'neu': 0.587, 'pos': 0.413, 'compound': 0.2755}\n",
      "vader\n",
      "{'neg': 0.0, 'neu': 0.587, 'pos': 0.413, 'compound': 0.2755}\n",
      "textblob\n",
      "{'Pol': 0.1, 'Subj': 0.4}\n",
      "api\n",
      "{'neg': 0.7710194829808472, 'neutral': 0.2550811696248651, 'pos': 0.22898051701915284}\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text, mode = 'textblob'):\n",
    "    if mode == 'textblob':\n",
    "        testimonial = TextBlob(text)\n",
    "        return {'Pol': testimonial.sentiment.polarity,\n",
    "                'Subj': testimonial.sentiment.subjectivity}\n",
    "    elif mode == 'nltk':\n",
    "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        return sid.polarity_scores(text)\n",
    "    elif mode == 'api':    \n",
    "        import requests   \n",
    "        # api-endpoint \n",
    "        URL = \"http://text-processing.com/api/sentiment/\"\n",
    "        params = {'text':text}\n",
    "        r = requests.post(url = URL, data = params)\n",
    "        data = r.json()\n",
    "        return(data['probability'])\n",
    "    elif mode == 'vader':\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        return analyzer.polarity_scores(text)        \n",
    "\n",
    "text = \"bernie couldn't sucks more\"\n",
    "for mode in ['nltk', 'vader', 'textblob', 'api']:\n",
    "    print(mode)\n",
    "    print(get_sentiment(text, mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet and Sentence Objects\n",
    "Let's makes some classes and methods for tweets and sentences that cleans and gets the sentiment of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence object\n",
    "class Sentence:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentiment = []\n",
    "        self.sentimentize()\n",
    "    \n",
    "    # calculate sentiment for the sentence\n",
    "    def sentimentize(self):\n",
    "        for mode in ['nltk', 'vader', 'textblob', 'api']:\n",
    "            self.sentiment.append((mode, get_sentiment(self.text, mode)))\n",
    "            \n",
    "    def __str__(self):\n",
    "        import json\n",
    "        sentiment_str = ''\n",
    "        for s in self.sentiment:\n",
    "            sentiment_str += json.dumps(s) + '\\n'\n",
    "        return '%s >>>>> \\n%s' % (self.text, sentiment_str)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s >>>>> Pol: %.1f (Sub: %.1f)' % (self.text, self.polarity, self.subjectivity)\n",
    "        \n",
    "# tweet object\n",
    "class Tweet:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentencize()\n",
    "        \n",
    "    # clean tweet and break down sentences\n",
    "    def sentencize(self):\n",
    "        self.sentences = [Sentence(t) for t in clean_tweet(self.text)]\n",
    "    \n",
    "    def disp(self):\n",
    "        print('********************')\n",
    "        print(self.text)\n",
    "        print('====================')\n",
    "        for sentence in self.sentences:\n",
    "            print(sentence)\n",
    "            print('--------------------')\n",
    "        print('********************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet downloader\n",
    "This downloads tweets to test clearning and sentiment analyis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST API\n",
    "import tweepy\n",
    "import sys\n",
    "\n",
    "def get_sample_tweets(query_phrase, tweet_count):\n",
    "    # authorization\n",
    "    auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True,           # wait until the limit is replenished\n",
    "                           wait_on_rate_limit_notify=True)    # reply with a message if the limit is reached\n",
    "\n",
    "    # check if not authorized\n",
    "    if (not api):\n",
    "        print (\"Can't Authenticate\")\n",
    "        return\n",
    "\n",
    "    tweets = []\n",
    "    for status in tweepy.Cursor(api.search, q = query_phrase,\n",
    "                                        tweet_mode = 'extended',\n",
    "                                        lang = 'en').items(tweet_count):\n",
    "        try:\n",
    "            full_text = status._json['retweeted_status']['full_text']\n",
    "        except:\n",
    "            full_text = status._json['full_text']\n",
    "        \n",
    "        tweets.append(Tweet(full_text))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Now let's download some tweets and test the *cleaning* and *sentiment analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "#BREAKING:\n",
      "\n",
      "Over 4,000 pieces of residue turn out to see Bernie Sanders in Norman, Oklahoma.\n",
      "\n",
      "Why is he still running? https://t.co/aMTvCZ8hyd\n",
      "====================\n",
      "breaking over 4 000 pieces of residue turn out to see bernie sanders in norman oklahoma >>>>> \n",
      "[\"nltk\", {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}]\n",
      "[\"vader\", {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}]\n",
      "[\"textblob\", {\"Pol\": 0.0, \"Subj\": 0.0}]\n",
      "[\"api\", {\"neg\": 0.5594764924650882, \"neutral\": 0.8934919806690461, \"pos\": 0.44052350753491176}]\n",
      "\n",
      "--------------------\n",
      "why is he still running >>>>> \n",
      "[\"nltk\", {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}]\n",
      "[\"vader\", {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}]\n",
      "[\"textblob\", {\"Pol\": 0.0, \"Subj\": 0.0}]\n",
      "[\"api\", {\"neg\": 0.7648456508299224, \"neutral\": 0.8193908813219645, \"pos\": 0.23515434917007758}]\n",
      "\n",
      "--------------------\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "tweets = get_sample_tweets(query_phrase = 'bernie sanders', tweet_count = 1)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet.disp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 100 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 100 tweets, Saved to Elizabeth_tweets.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "\n",
    "searchQuery = '#someHashtag'  # this is what we're searching for\n",
    "searchQuery = 'Elizabeth Warren'  # this is what we're searching for\n",
    "maxTweets = 100 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = '%s_tweets.txt' % searchQuery.split(\" \")[0] # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                        '\\n')\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print(\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STREAMING\n",
    "\n",
    "import tweepy\n",
    "import json\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "auth = OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACS_TOKEN, ACS_SECRET)\n",
    "api = API(auth, wait_on_rate_limit=True,\n",
    "                wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "# Continue with rest of code\n",
    "\n",
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, max_count = 5):\n",
    "        self.count = 0\n",
    "        self.max_count = max_count\n",
    "\n",
    "    def on_data(self, data):\n",
    "        all_data = json.loads(data)\n",
    "        print('*************')\n",
    "        print(self.count)\n",
    "        print('*************')\n",
    "        #print('-----------')\n",
    "        print(all_data['text'])\n",
    "        print('||||||||')\n",
    "        try:\n",
    "            if 'retweeted_status' in all_data:\n",
    "                print('RETWEET')\n",
    "                try:\n",
    "                    print(all_data['retweeted_status']['extended_tweet']['full_text'])\n",
    "                except:\n",
    "                    print('SHORT TWEET')\n",
    "                    print(all_data['retweeted_status']['text'])\n",
    "            else:\n",
    "                print('NOT A RETWEET!!')\n",
    "                try:\n",
    "                    print(all_data['extended_tweet']['full_text'])\n",
    "                except:\n",
    "                    print('SHORT TWEET')\n",
    "                    print(all_data['text'])\n",
    "                    \n",
    "        except:\n",
    "            for k, v in all_data.items():\n",
    "                print(k, v)\n",
    "            return False\n",
    "        \n",
    "        #\n",
    "        #try:\n",
    "        #    tweet = all_data['extended_tweet']['full_text']\n",
    "        #except:\n",
    "        #    tweet = all_data['full_text']\n",
    "        \n",
    "        #coded = tweet.encode('utf-8')\n",
    "        #tweet = str(coded)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tweet = all_data['retweeted_status']['extended_tweet']['full_text']\n",
    "        except:\n",
    "            try:\n",
    "                tweet = all_data['retweeted_status']['text']\n",
    "            except:\n",
    "                try:\n",
    "                    tweet = all_data['extended_tweet']['full_text']\n",
    "                except:\n",
    "                    try:\n",
    "                        tweet = all_data['text']\n",
    "                    except:\n",
    "                        for k, v in all_data.items():\n",
    "                            print(k, v)\n",
    "        \"\"\"\n",
    "        #print(tweet)\n",
    "            \n",
    "        self.count += 1\n",
    "        if self.count > self.max_count:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print('ERR!!')\n",
    "        print(status)\n",
    "        if status == 420:\n",
    "            return False\n",
    "        \n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "        \n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to query both names and @**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start streaming.\n",
      "*************\n",
      "0\n",
      "*************\n",
      "@BernieSanders We do lead the way: https://t.co/AazkhH1cRw\n",
      "||||||||\n",
      "NOT A RETWEET!!\n",
      "SHORT TWEET\n",
      "@BernieSanders We do lead the way: https://t.co/AazkhH1cRw\n",
      "*************\n",
      "1\n",
      "*************\n",
      "RT @United4Kamala: üèÅ FRIDAY FUNDRAISER üèÅ\n",
      "\n",
      "Tonight @KamalaHarris sets üî• to the #DemocraticDebate stage.\n",
      "\n",
      "Tomorrow morning - 8am ET - we fund‚Ä¶\n",
      "||||||||\n",
      "RETWEET\n",
      "üèÅ FRIDAY FUNDRAISER üèÅ\n",
      "\n",
      "Tonight @KamalaHarris sets üî• to the #DemocraticDebate stage.\n",
      "\n",
      "Tomorrow morning - 8am ET - we fundraise!\n",
      "\n",
      "‚ÄúDedication, hard work, plus patience.‚Äù #NipseyHussle \n",
      "\n",
      "Please donate tonight and/or tomorrow!\n",
      "\n",
      "#KHive #KamalaHarris2020 \n",
      "https://t.co/El1ePxzkat https://t.co/r6DXeGMtEH\n",
      "*************\n",
      "2\n",
      "*************\n",
      "@davidaxelrod @BarackObama @JoeBiden I don‚Äôt want any of these bitches attacking Obama! ü§¨\n",
      "||||||||\n",
      "NOT A RETWEET!!\n",
      "SHORT TWEET\n",
      "@davidaxelrod @BarackObama @JoeBiden I don‚Äôt want any of these bitches attacking Obama! ü§¨\n",
      "*************\n",
      "3\n",
      "*************\n",
      "RT @HillaryPix: When T.L. Duryea is unsure how to proceed, in politics or in life, she asks herself a question:\n",
      "‚ÄúWhat would Hillary Clinton‚Ä¶\n",
      "||||||||\n",
      "RETWEET\n",
      "When T.L. Duryea is unsure how to proceed, in politics or in life, she asks herself a question:\n",
      "‚ÄúWhat would Hillary Clinton want me to do?‚Äù\n",
      "@TinaDuryea\n",
      "‚ÄúIn my heart, I think Hillary wants Kamala Harris to win.‚Äù \n",
      "https://t.co/lsQUJ9ntnM\n",
      "*************\n",
      "4\n",
      "*************\n",
      "Bernie Sanders BEEN KNOWN. WE NEED HIM\n",
      "||||||||\n",
      "NOT A RETWEET!!\n",
      "SHORT TWEET\n",
      "Bernie Sanders BEEN KNOWN. WE NEED HIM\n",
      "*************\n",
      "5\n",
      "*************\n",
      "RT @ArchKennedy: .@BernieSanders attended an ISNA convention with Islamic supremacists who back killing of gay people.\n",
      "\n",
      "Will MSM report on‚Ä¶\n",
      "||||||||\n",
      "RETWEET\n",
      ".@BernieSanders attended an ISNA convention with Islamic supremacists who back killing of gay people.\n",
      "\n",
      "Will MSM report on this? \n",
      "\n",
      "As a gay man, I am HIGHLY concerned about a Presidential candidate supporting a group that is ok with the murder of LGBTQ!\n",
      "\n",
      "https://t.co/qnhZxbxHPn\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Start streaming.')\n",
    "    myStream.filter(track = mentions, languages=['en'])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped.\")\n",
    "finally:\n",
    "    print('Done.')\n",
    "    myStream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
