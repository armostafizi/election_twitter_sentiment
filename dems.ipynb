{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Democratic Candidates\n",
    "This notebook analyzes the sentiment of the tweets posted for democratic candidates. There are a few ideas that i need to consider. For instance,\n",
    "\n",
    "1. The number of positive and negative tweets posted for each candidate\n",
    "2. The proportion of positivity and negativity\n",
    "3. How these change over time and possibly after each debate or major event\n",
    "4. Is there any relationship between the tweets sentiments and the pols?\n",
    "5. The location of the sentiments, broken down to the states, possibly focusing on the swing states.\n",
    "6. We can expand the analysis beyond the tweets and to the *users/voters*.\n",
    "7. Make a word cloud for the tweets about each candidate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweeter Data\n",
    "I start with pulling some data from the major candidates from twitter, Elizabeth Warren, Bernine Sanders, and Joe Biden. For this, I used *tweepy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch api keys from api_keys.py\n",
    "\n",
    "from api_keys import *\n",
    "\n",
    "query = {'@BernieSanders': 'Bernie Sanders',     # Bernie\n",
    "         '@ewarren':       'Elizabeth Warren',   # Elizabeth\n",
    "         '@KamalaHarris':  'Kamala Harris',      # Kamala\n",
    "         '@PeteButtigieg': 'Pete Buttigieg',     # Pete\n",
    "         '@JoeBiden':      'Joe Biden',          # Joe Biden\n",
    "         }\n",
    "\n",
    "query = dict((k.lower(), v.lower()) for k,v in query.items())\n",
    "\n",
    "# make everything lowercase for consitency\n",
    "mentions = list(query.keys())\n",
    "names = list(query.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Things that are need to be taken care of in data cleaning.\n",
    "\n",
    "* Remove links (https, etc.).\n",
    "* Remove pnctuations except dots and commas. So later we break them down accordingly. Or maybe *tokenize* does that automatically? Need to check!\n",
    "* Remove special characters\n",
    "* Remove numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the tweet\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as pp\n",
    "#nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions\n",
    "\n",
    "# only remove URL, reserved words, emojies, and smilies. Preserve hashtags and mentions\n",
    "pp.set_options(pp.OPT.URL, pp.OPT.RESERVED, pp.OPT.EMOJI, pp.OPT.SMILEY)\n",
    "# save stop words to be removed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# nltk tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# For the contractions\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "\n",
    "# remove mentions if not a candidate\n",
    "def remove_mentions(tweet, mentions):\n",
    "    words = tweet.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.startswith('@'):\n",
    "            candidate_mentioned = False\n",
    "            for m in mentions:\n",
    "                if m in w:\n",
    "                    candidate_mentioned = True\n",
    "                    clean_words.append(w.replace(m, query[m].lower()))\n",
    "                    break\n",
    "            if not candidate_mentioned:\n",
    "                clean_words.append(w[1:]) # remove @\n",
    "        else:\n",
    "            clean_words.append(w)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "# expand hashtags\n",
    "def fix_hashtags(tweet):\n",
    "    # first replace underscore with space\n",
    "    tweet = tweet.replace('_', ' ')\n",
    "    words = tweet.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        if w.startswith('#'):\n",
    "            w = ' '.join([a for a in re.split('([A-Z][a-z]+)', w[1:]) if a])\n",
    "        clean_words.append(w)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "def expand_contractions(tweet):\n",
    "    def replace(match):\n",
    "        # expand the contraction with the most possible alternative : [0]\n",
    "        return contractions[match.group(0)][0]\n",
    "    return contractions_re.sub(replace, tweet)\n",
    "    \n",
    "def clean_tweet(tweet):\n",
    "    # remove URL, Reserved words (RT, FAV, etc.), Emojies, Smilies, and Numbers.\n",
    "    # preserve mentions and hastags for now\n",
    "    tweet = pp.clean(tweet)\n",
    "    # fix hashtags\n",
    "    tweet = fix_hashtags(tweet)\n",
    "    # make the tweet lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # now remove mentions that are not the candidates\n",
    "    tweet = remove_mentions(tweet, mentions)\n",
    "    # conver U+2019 to U+0027 (apostrophe)\n",
    "    tweet = tweet.replace(u\"\\u2019\", u\"\\u0027\")\n",
    "    # expand the contractions\n",
    "    tweet = expand_contractions(tweet)\n",
    "    # remove 's\n",
    "    tweet = tweet.replace(\"'s\",'')\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    # break into sentences\n",
    "    # tb = TextBlob(tweet)\n",
    "    sentences = []\n",
    "    for sent in tokenizer.tokenize(tweet):\n",
    "#    for sent in tb.sentences: # for this punkt package of nltk has to be downloaded once\n",
    "#                              # with the following code:\n",
    "#                              # import nltk\n",
    "#                              # nltk.download('punkt')\n",
    "        sent = str(sent)\n",
    "        # remove ponctuations\n",
    "        sent = sent.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        # consolidate white spaces\n",
    "        sent = ' '.join(sent.split())\n",
    "        if len(sent) > 4: # if the sentence is larger than 4 chars\n",
    "            sentences.append(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Each tweet might comprise multiple sentences. Therefore each tweet must be broken down to different sentences with *tokenize* functionality of *TextBlob*. The final sentiment can be a function of the sentiment of different sentences, perhaps the average (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# This needs more work to be more accurate. Some ideas:\n",
    "#    1. Don't remove emoticons and use vader\n",
    "#    2. clean stopwrds and everything else, and use outofthebox texblob\n",
    "#    3. Read papers on political sentiment analysis with twitter\n",
    "def get_sentiment(text, mode = 'textblob'):\n",
    "    if mode == 'textblob':\n",
    "        testimonial = TextBlob(text)\n",
    "        return {'pol': testimonial.sentiment.polarity,\n",
    "                'subj': testimonial.sentiment.subjectivity}\n",
    "    elif mode == 'nltk':\n",
    "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        return sid.polarity_scores(text)\n",
    "    elif mode == 'api':    \n",
    "        import requests   \n",
    "        # api-endpoint \n",
    "        URL = \"http://text-processing.com/api/sentiment/\"\n",
    "        params = {'text':text}\n",
    "        r = requests.post(url = URL, data = params)\n",
    "        data = r.json()\n",
    "        return(data['probability'])\n",
    "    elif mode == 'vader':\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        return analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet and Sentence Objects\n",
    "Let's makes some classes and methods for tweets and sentences that cleans and gets the sentiment of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence object\n",
    "import numpy as np\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentiment = []\n",
    "        self.sentimentize()\n",
    "    \n",
    "    # calculate sentiment for the sentence\n",
    "    def sentimentize(self, compare = True):\n",
    "        if compare:\n",
    "            self.sentiment = dict()\n",
    "            for mode in ['nltk', 'vader', 'textblob', 'api']:\n",
    "                self.sentiment[mode] = get_sentiment(self.text, mode)\n",
    "        else:\n",
    "            self.sentiment = {'textblob': get_sentiment(self.text, mode = 'textblob')}\n",
    "            \n",
    "    def __str__(self):\n",
    "        import json\n",
    "        sentiment_str = ''\n",
    "        for s in self.sentiment:\n",
    "            sentiment_str += s + ': ' + json.dumps(self.sentiment[s]) + '\\n'\n",
    "        return '%s >>>>> \\n%s' % (self.text, sentiment_str)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s >>>>> Pol: %.1f (Sub: %.1f)' % (self.text, self.polarity, self.subjectivity)\n",
    "        \n",
    "# tweet object\n",
    "class Tweet:\n",
    "    def __init__(self, text, time):\n",
    "        self.time = time\n",
    "        self.text = text\n",
    "        self.sentiment = None\n",
    "        self.sentencize()\n",
    "        \n",
    "    # clean tweet and break down sentences\n",
    "    def sentencize(self):\n",
    "        self.sentences = [Sentence(t) for t in clean_tweet(self.text)]\n",
    "        # tweet snetiment is the avergae sentiment of all sentences. TODO: Might not be correct!\n",
    "        self.sentiment = np.mean([s.sentiment['textblob']['pol'] for s in self.sentences])\n",
    "    \n",
    "    def disp(self):\n",
    "        print('********************')\n",
    "        print(self.text)\n",
    "        print('====================')\n",
    "        for sentence in self.sentences:\n",
    "            print(sentence)\n",
    "            print('--------------------')\n",
    "        print('Compount Sentiment: %.1f' % self.sentiment)\n",
    "        print('********************')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet downloader\n",
    "This downloads tweets to test clearning and sentiment analyis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST API\n",
    "import tweepy\n",
    "import sys\n",
    "\n",
    "def get_sample_tweets(query_phrase, tweet_count):\n",
    "    # authorization\n",
    "    auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True,           # wait until the limit is replenished\n",
    "                           wait_on_rate_limit_notify=True)    # reply with a message if the limit is reached\n",
    "\n",
    "    # check if not authorized\n",
    "    if (not api):\n",
    "        print (\"Can't Authenticate\")\n",
    "        return\n",
    "\n",
    "    tweets = []\n",
    "    for status in tweepy.Cursor(api.search, q = query_phrase,\n",
    "                                        tweet_mode = 'extended',\n",
    "                                        lang = 'en').items(tweet_count):\n",
    "        try:\n",
    "            full_text = status._json['retweeted_status']['full_text']\n",
    "        except:\n",
    "            full_text = status._json['full_text']\n",
    "            \n",
    "        ts = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(status._json['created_at'],'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "        tweets.append(Tweet(full_text, ts))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Now let's download some tweets and test the *cleaning* and *sentiment analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "@rweingarten @rmc031 @BernieSanders .@UTLAnow leadership has courage; employing true democratic transparency &amp; pushing progress instead of lowering expectations &amp; dampening movements.\n",
      "\n",
      "Weak AFT &amp; NEA leadership is why we are where we are...and now need the boldness of #Bernie2020\n",
      "\n",
      "#AFTvotes\n",
      "\n",
      "https://t.co/slzkybsDum\n",
      "====================\n",
      "rweingarten rmc031 bernie sanders >>>>> \n",
      "nltk: {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}\n",
      "vader: {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0, \"compound\": 0.0}\n",
      "textblob: {\"pol\": 0.0, \"subj\": 0.0}\n",
      "api: {\"neg\": 0.5204076178484814, \"neutral\": 0.6575910835047992, \"pos\": 0.47959238215151856}\n",
      "\n",
      "--------------------\n",
      "utlanow leadership has courage employing true democratic transparency amp pushing progress instead of lowering eectations amp dampening movements >>>>> \n",
      "nltk: {\"neg\": 0.081, \"neu\": 0.565, \"pos\": 0.355, \"compound\": 0.7783}\n",
      "vader: {\"neg\": 0.081, \"neu\": 0.565, \"pos\": 0.355, \"compound\": 0.7783}\n",
      "textblob: {\"pol\": 0.35, \"subj\": 0.65}\n",
      "api: {\"neg\": 0.49124714712506456, \"neutral\": 0.5173807779065595, \"pos\": 0.5087528528749354}\n",
      "\n",
      "--------------------\n",
      "weak aft amp nea leadership is why we are where we are and now need the boldness of bernie 2020 af tvotes >>>>> \n",
      "nltk: {\"neg\": 0.114, \"neu\": 0.787, \"pos\": 0.098, \"compound\": -0.1027}\n",
      "vader: {\"neg\": 0.114, \"neu\": 0.787, \"pos\": 0.098, \"compound\": -0.1027}\n",
      "textblob: {\"pol\": -0.375, \"subj\": 0.625}\n",
      "api: {\"neg\": 0.7720139719950063, \"neutral\": 0.2216920434264719, \"pos\": 0.22798602800499373}\n",
      "\n",
      "--------------------\n",
      "Compount Sentiment: -0.0\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "tweets = get_sample_tweets(query_phrase = 'bernie sanders', tweet_count = 1)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet.disp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "Now let's stream the tweets in real time and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STREAMING\n",
    "\n",
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Stream\n",
    "auth = OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACS_TOKEN, ACS_SECRET)\n",
    "api = API(auth, wait_on_rate_limit=True,\n",
    "                wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "# Continue with rest of code\n",
    "\n",
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, max_count = 5, verbose = False):\n",
    "        self.count = 0\n",
    "        self.verbose = verbose\n",
    "        self.max_count = max_count\n",
    "        \n",
    "    def on_data(self, data):\n",
    "        all_data = json.loads(data)\n",
    "        try:\n",
    "            if 'retweeted_status' in all_data:\n",
    "                try:\n",
    "                    text = all_data['retweeted_status']['extended_tweet']['full_text']\n",
    "                except:\n",
    "                    text = all_data['retweeted_status']['text']\n",
    "            else:\n",
    "                try:\n",
    "                    text = all_data['extended_tweet']['full_text']\n",
    "                except:\n",
    "                    text = all_data['text']\n",
    "        except:\n",
    "            print('New structure!')\n",
    "            \n",
    "        ts = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(all_data['created_at'],'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "        tweet = Tweet(text, ts)\n",
    "        \n",
    "        print('%s : \\t%.1f' % (str(tweet.time), tweet.sentiment))\n",
    "\n",
    "        if self.verbose:\n",
    "            print('*************')\n",
    "            print(self.count)\n",
    "            print('*************')\n",
    "            print(all_data['text'])\n",
    "            print('||||||||')\n",
    "            tweet.disp()\n",
    "        \n",
    "        self.count += 1\n",
    "        if self.count > self.max_count:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print('ERR!!')\n",
    "        print(status)\n",
    "        if status == 420:\n",
    "            return False\n",
    "        \n",
    "    def on_status(self, status):\n",
    "        print(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-24 02:03:36\n",
      "*************\n",
      "0\n",
      "*************\n",
      "RT @TyWorley2: So Elizabeth Warren was asked about Mollie Tibbetts being murdered by an illegal...\n",
      "\n",
      "Her reply was: \n",
      "\n",
      "“I know this is hard f…\n",
      "||||||||\n",
      "********************\n",
      "So Elizabeth Warren was asked about Mollie Tibbetts being murdered by an illegal...\n",
      "\n",
      "Her reply was: \n",
      "\n",
      "“I know this is hard for her family, but they have to remember that we need to focus on real problems like illegal immigrants not being able to see their kids”\n",
      "\n",
      "Let that sink in.\n",
      "====================\n",
      "so elizabeth warren was asked about mollie tibbetts being murdered by an illegal her reply was i know this is hard for her family but they have to remember that we need to focus on real problems like illegal immigrants not being able to see their kids let that sink in >>>>> \n",
      "[\"textblob\", {\"pol\": -0.11833333333333336, \"subj\": 0.49333333333333335}]\n",
      "\n",
      "--------------------\n",
      "********************\n",
      "2019-09-24 02:03:36\n",
      "*************\n",
      "1\n",
      "*************\n",
      "RT @sharszone1420: ON HANNITY....\n",
      "\n",
      "UKRANINAN PROSECUTOR WHO WAS FIRED VIA ORDERS FROM CREEPY,SLEAZY, joe.....STATED HE WAS ABOUT TO INTERVI…\n",
      "||||||||\n",
      "********************\n",
      "ON HANNITY....\n",
      "\n",
      "UKRANINAN PROSECUTOR WHO WAS FIRED VIA ORDERS FROM CREEPY,SLEAZY, joe.....STATED HE WAS ABOUT TO INTERVIEW hunter biden WHEN HE GOT FIRED.\n",
      "\n",
      "ROH...ROH..joe\n",
      "\n",
      "STILL THINKING PRES TRUMP/GUILIANI SET THIS ALL UP TO TRICK THE demonrats INTO DEMANDING AN INVESTIGATION.\n",
      "====================\n",
      "on hannity ukraninan prosecutor who was fired via orders from creepy sleazy joe stated he was about to interview hunter biden when he got fired >>>>> \n",
      "[\"textblob\", {\"pol\": -0.5, \"subj\": 1.0}]\n",
      "\n",
      "--------------------\n",
      "roh roh joe still thinking pres trump guiliani set this all up to trick the demonrats into demanding an investigation >>>>> \n",
      "[\"textblob\", {\"pol\": 0.0, \"subj\": 0.0}]\n",
      "\n",
      "--------------------\n",
      "********************\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "max_count = 1      \n",
    "myStreamListener = MyStreamListener(max_count = max_count, verbose = True)\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n",
    "\n",
    "try:\n",
    "    myStream.filter(track = mentions + names, languages=['en'])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped.\")\n",
    "finally:\n",
    "    print('Done.')\n",
    "    myStream.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam and plot\n",
    "Try streaming and plotting the results on the go for **Bernie Sanders** only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-24 02:18:33 : \t0.5\n",
      "2019-09-24 02:18:34 : \t0.2\n",
      "2019-09-24 02:18:35 : \t0.0\n",
      "2019-09-24 02:18:36 : \t0.1\n",
      "2019-09-24 02:18:37 : \t-0.3\n",
      "2019-09-24 02:18:37 : \t0.0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "max_count = 5\n",
    "myStreamListener = MyStreamListener(max_count = max_count)\n",
    "myStream = tweepy.Stream(auth = api.auth, listener = myStreamListener)\n",
    "\n",
    "try:\n",
    "    myStream.filter(track = ['Bernie Sanders', '@berniesanders'], languages=['en'])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped.\")\n",
    "finally:\n",
    "    print('Done.')\n",
    "    myStream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
